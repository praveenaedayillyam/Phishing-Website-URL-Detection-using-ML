{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PHISHING SITE URL DETECTION FINAL RESULT**"
      ],
      "metadata": {
        "id": "KfkgMfdzeDA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mounting Drive**"
      ],
      "metadata": {
        "id": "clCgcjrqZ7pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpL5ZtWWObgG",
        "outputId": "5d55529b-9c58-4369-a9f8-52ee4e905e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries**"
      ],
      "metadata": {
        "id": "zdh-YF16aEQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import urllib.parse\n",
        "import re\n",
        "import string"
      ],
      "metadata": {
        "id": "kIvvXyALTo0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading models from Pickle File**"
      ],
      "metadata": {
        "id": "WJrjvqxoaQaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/PHYSHING SITE PROJECT/FINALL_PHISHING.pkl', 'rb') as f:\n",
        "    loaded_objects = pickle.load(f)\n",
        "\n",
        "RF_model_num = loaded_objects['RF_model_num']\n",
        "LR_model_text = loaded_objects['LR_model_text']\n",
        "vectorizer = loaded_objects['vectorizer']\n",
        "scaler = loaded_objects['scaler']"
      ],
      "metadata": {
        "id": "-uv9bu2oTu8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Extraction and Preprocessing of given input**"
      ],
      "metadata": {
        "id": "1saFwuWwaivP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_url_features(url):\n",
        "    # Parsing the URL\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "\n",
        "    # Extracting URL features\n",
        "    url_length = len(url)\n",
        "    domain = parsed_url.netloc\n",
        "    domain_length = len(domain)\n",
        "    tld = domain.split('.')[-1]\n",
        "    tld_length = len(tld)\n",
        "    subdomains = parsed_url.hostname.split('.')\n",
        "\n",
        "    # Removing www\n",
        "    if subdomains[0] == 'www':\n",
        "        subdomains.pop(0)\n",
        "\n",
        "    if len(subdomains) > 1:\n",
        "        tld = subdomains.pop(-1)  # Removing TLD if it exists\n",
        "\n",
        "    # Calculating the number of subdomains\n",
        "    no_of_subdomains = len(subdomains)\n",
        "    path = parsed_url.path\n",
        "\n",
        "    # Counting the number of letters and digits in the URL\n",
        "    letters_in_url = sum(c.isalpha() for c in url)\n",
        "    digits_in_url = sum(c.isdigit() for c in url)\n",
        "\n",
        "    # Counting special characters\n",
        "    equals_count = url.count('=')\n",
        "    qmark_count = url.count('?')\n",
        "    ampersand_count = url.count('&')\n",
        "    other_special_chars_count = len(re.findall(r'[^a-zA-Z0-9?&=./]', url))\n",
        "\n",
        "    # calculationg ratios\n",
        "    letter_ratio = letters_in_url / url_length\n",
        "    digit_ratio = digits_in_url / url_length\n",
        "    special_char_ratio = other_special_chars_count / url_length\n",
        "\n",
        "    # Checking isHTTPS\n",
        "    if parsed_url.scheme == 'https':\n",
        "        is_https = 1\n",
        "    else:\n",
        "        is_https = 0\n",
        "\n",
        "    # Returning extracted features\n",
        "    return {\n",
        "        'URLLength': url_length,\n",
        "        'DomainLength': domain_length,\n",
        "        'TLDLength': tld_length,\n",
        "        'NoOfSubDomain': no_of_subdomains,\n",
        "        'NoOfLettersInURL': letters_in_url,\n",
        "        'LetterRatioInURL': letter_ratio,\n",
        "        'NoOfDegitsInURL': digits_in_url,\n",
        "        'DegitRatioInURL': digit_ratio,\n",
        "        'NoOfEqualsInURL': equals_count,\n",
        "        'NoOfQMarkInURL': qmark_count,\n",
        "        'NoOfAmpersandInURL': ampersand_count,\n",
        "        'NoOfOtherSpecialCharsInURL': other_special_chars_count,\n",
        "        'SpacialCharRatioInURL': special_char_ratio,\n",
        "        'IsHTTPS': is_https\n",
        "    }\n",
        "\n",
        "def split_by_punc(url):\n",
        "    url_parts = re.split(r'([{}])'.format(re.escape(string.punctuation)), url)\n",
        "    url_joined = ' '.join(url_parts)\n",
        "    return url_joined\n",
        "\n",
        "def remove_punctuations(url):\n",
        "    clean_url = ''.join(char for char in url if char not in string.punctuation)\n",
        "    return clean_url"
      ],
      "metadata": {
        "id": "AAo2fe_AT1D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function For Prediction**"
      ],
      "metadata": {
        "id": "zffXVIlhb8sG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_and_probability(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Find all <a> tags with 'href' attribute pointing to another webpage\n",
        "        external_refs = soup.find_all('a', href=True)\n",
        "        # Count the number of external references\n",
        "        num_external_refs = len(external_refs)\n",
        "\n",
        "        # Extracting numerical features from the URL\n",
        "        numerical_features = extract_url_features(url)\n",
        "        numerical_features_df = pd.DataFrame([numerical_features])\n",
        "        numerical_features_df['NoOfExternalRef'] = num_external_refs\n",
        "        numerical_features_scaled = scaler.transform(numerical_features_df)\n",
        "\n",
        "        # Extracting text features from the URL\n",
        "        clean_url = split_by_punc(url)\n",
        "        clean_url = remove_punctuations(clean_url).lower()\n",
        "        clean_url = clean_url.replace('www', '')\n",
        "        text_features = vectorizer.transform([clean_url])\n",
        "\n",
        "        # probability predictions using both models\n",
        "        proba_num = RF_model_num.predict_proba(numerical_features_scaled)[0, 1]  # Probability of positive class\n",
        "        proba_text = LR_model_text.predict_proba(text_features)[0, 1]  # Probability of positive class\n",
        "\n",
        "        # Combining the probabilities using averaging\n",
        "        avg_proba = (proba_num + proba_text) / 2\n",
        "        return avg_proba\n",
        "    except Exception as e:\n",
        "        print(f\"Error predicting probability: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "wGEQKh6sT9nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PHISHING SITE URL PREDICTION**"
      ],
      "metadata": {
        "id": "BtXKv-CIcNkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n---------------------------------------- PHISHING SITE DETECION ----------------------------------------\")\n",
        "print()\n",
        "\n",
        "# Input field for entering the URL\n",
        "url = input('Enter the URL of the website:')\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\\n\\n\")\n",
        "\n",
        "probability = prediction_and_probability(url)\n",
        "\n",
        "if probability >= 0.4:\n",
        "    print(f\"The URL is predicted to be GOOD \\n(probability: {probability:.2f})\")\n",
        "else:\n",
        "    print(f\"The URL is predicted to be PHISHING \\n(probability: {probability:.2f})\")\n",
        "\n",
        "print(\"\\n\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CgB_v9LT-bz",
        "outputId": "c9251c9c-67ee-4c30-dff2-8b9b61e53768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "---------------------------------------- PHISHING SITE DETECION ----------------------------------------\n",
            "\n",
            "Enter the URL of the website:https://meet.google.com\n",
            "\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "\n",
            "\n",
            "The URL is predicted to be GOOD \n",
            "(probability: 0.48)\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}